export const dummyContent = [
  {
    title: "Lorem Ipsum Dolor Sit Amet",
    description: (
      <>
        <p className="text-content">
          Sit duis est minim proident non nisi velit non consectetur. Esse
          adipisicing laboris consectetur enim ipsum reprehenderit eu deserunt
          Lorem ut aliqua anim do. Duis cupidatat qui irure cupidatat incididunt
          incididunt enim magna id est qui sunt fugiat. Laboris do duis pariatur
          fugiat Lorem aute sit ullamco. Qui deserunt non reprehenderit dolore
          nisi velit exercitation Lorem qui do enim culpa. Aliqua eiusmod in
          occaecat reprehenderit laborum nostrud fugiat voluptate do Lorem culpa
          officia sint labore. Tempor consectetur excepteur ut fugiat veniam
          commodo et labore dolore commodo pariatur.
        </p>
        <p className="text-content">
          Dolor minim irure ut Lorem proident. Ipsum do pariatur est ad ad
          veniam in commodo id reprehenderit adipisicing. Proident duis
          exercitation ad quis ex cupidatat cupidatat occaecat adipisicing.
        </p>
        <p className="text-content">
          Tempor quis dolor veniam quis dolor. Sit reprehenderit eiusmod
          reprehenderit deserunt amet laborum consequat adipisicing officia qui
          irure id sint adipisicing. Adipisicing fugiat aliqua nulla nostrud.
          Amet culpa officia aliquip deserunt veniam deserunt officia
          adipisicing aliquip proident officia sunt.
        </p>
      </>
    ),
    badge: "React",
    image: "/vegeta.jpeg",
  },
  {
    title: "Lorem Ipsum Dolor Sit Amet",
    description: (
      <>
        <p className="text-content">
          Ex irure dolore veniam ex velit non aute nisi labore ipsum occaecat
          deserunt cupidatat aute. Enim cillum dolor et nulla sunt exercitation
          non voluptate qui aliquip esse tempor. Ullamco ut sunt consectetur
          sint qui qui do do qui do. Labore laborum culpa magna reprehenderit ea
          velit id esse adipisicing deserunt amet dolore. Ipsum occaecat veniam
          commodo proident aliqua id ad deserunt dolor aliquip duis veniam sunt.
        </p>
        <p className="text-content">
          In dolore veniam excepteur eu est et sunt velit. Ipsum sint esse
          veniam fugiat esse qui sint ad sunt reprehenderit do qui proident
          reprehenderit. Laborum exercitation aliqua reprehenderit ea sint
          cillum ut mollit.
        </p>
      </>
    ),
    badge: "Changelog",
    image: "/vegeta.jpeg",
  },
  {
    title: "Lorem Ipsum Dolor Sit Amet",
    description: (
      <>
        <p className="text-content">
          Ex irure dolore veniam ex velit non aute nisi labore ipsum occaecat
          deserunt cupidatat aute. Enim cillum dolor et nulla sunt exercitation
          non voluptate qui aliquip esse tempor. Ullamco ut sunt consectetur
          sint qui qui do do qui do. Labore laborum culpa magna reprehenderit ea
          velit id esse adipisicing deserunt amet dolore. Ipsum occaecat veniam
          commodo proident aliqua id ad deserunt dolor aliquip duis veniam sunt.
        </p>
      </>
    ),
    badge: "Launch Week",
    image: "/vegeta.jpeg",
  },
];

export const webDevContent = [
  {
    title: "www.amplifydentistry.com",
    description: (
      <>
        <p className="text-content">
          In recent years, the field of dentistry has witnessed significant
          advancements in technology, with artificial intelligence (AI) emerging
          as a powerful tool to enhance learning and practice. One such
          groundbreaking innovation is Amplify Dentistry, a platform
          revolutionizing dental education by harnessing the capabilities of AI.
          Amplify Dentistry is not just a learning tool; it&apos;s a
          comprehensive ecosystem designed to empower dental students,
          educators, and practitioners alike. This essay explores how Amplify
          Dentistry is reshaping dental education and equipping the next
          generation of dentists with the skills and knowledge they need to
          excel in their careers.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          At its core, Amplify Dentistry leverages AI to provide personalized
          learning experiences tailored to each student&apos;s unique needs and
          learning style. Through sophisticated algorithms and machine learning
          techniques, the platform analyzes students performance, identifies
          areas of strength and weakness, and delivers targeted educational
          content to address gaps in knowledge. This adaptive learning approach
          ensures that students receive the support and guidance they need to
          master key concepts and skills at their own pace.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          One of the key features of Amplify Dentistry is its interactive
          simulations and virtual patient cases. These immersive experiences
          allow students to practice clinical procedures in a realistic virtual
          environment, without the need for expensive equipment or access to
          live patients. By simulating various scenarios, from routine cleanings
          to complex surgical procedures, Amplify Dentistry provides students
          with invaluable hands-on experience and helps bridge the gap between
          theory and practice.
        </p>
      </>
    ),
    badge: "Built on NEXTJS",
    image: "/ampdent_logo.png",
  },
  {
    title: "Also Called AmpDent",
    description: (
      <>
        <p className="text-content">
          Moreover, Amplify Dentistry offers a wealth of educational resources,
          including video tutorials, case studies, and interactive quizzes,
          curated by leading experts in the field of dentistry. These resources
          cover a wide range of topics, from anatomy and physiology to advanced
          treatment techniques, ensuring that students receive a comprehensive
          education that prepares them for real-world challenges. Additionally,
          the platform fosters collaboration and knowledge sharing among
          students and educators, creating a vibrant learning community where
          ideas are exchanged and best practices are shared.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          Beyond its role in student education, Amplify Dentistry also offers
          benefits for dental educators and practitioners. The platform provides
          instructors with valuable insights into students progress and
          performance, allowing them to identify areas for improvement and
          tailor their teaching strategies accordingly. Additionally, Amplify
          Dentistry can be used as a tool for continuous professional
          development, enabling practitioners to stay updated on the latest
          advancements in dentistry and refine their skills through ongoing
          training and education.
        </p>
      </>
    ),
    badge: "Deployed on Vercel",
    image: "/coursebook.png",
  },
  {
    title: "Finishing Thoughts",
    description: (
      <>
        <p className="text-content">
          In conclusion, Amplify Dentistry represents a paradigm shift in dental
          education, leveraging AI to create personalized, immersive learning
          experiences that empower students to succeed. By combining adaptive
          learning algorithms, interactive simulations, and curated educational
          content, Amplify Dentistry is redefining the way dentistry is taught
          and learned. As the field of dentistry continues to evolve, platforms
          like Amplify Dentistry will play an increasingly vital role in shaping
          the future of dental education and ensuring that students are equipped
          with the knowledge and skills they need to thrive in their careers.
        </p>
      </>
    ),
    badge: "Current Stage: Public Beta",
    image: "/vegeta.jpeg",
  },
];

export const interfacesContent = [
  {
    title: "Inspired Designs",
    description: (
      <>
        <hr className="my-10" />
        <p className="text-content">
          Pankh.AI&apos;s interface design services epitomize the fusion of
          innovation and user-centricity. At the core of our approach lies a
          deep commitment to honesty, integrity, and customer satisfaction,
          values that transcend mere profit margins. Our interface design
          services are not just about creating visually appealing layouts; they
          are about crafting intuitive and immersive experiences that resonate
          with users on a profound level.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          We understand that effective interface design goes beyond aesthetics;
          it&apos;s about understanding user behavior, anticipating their needs,
          and guiding them seamlessly through the digital landscape. That&apos;s
          why our team of experienced designers employs a human-centered
          approach, placing the user at the center of the design process.
          Through meticulous research and iterative prototyping, we strive to
          create interfaces that are not only visually stunning but also highly
          functional and user-friendly.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          Moreover, at Pankh.AI, we believe in the power of transparency and
          open communication. We value customer feedback above all else,
          recognizing it as an invaluable source of insight and inspiration.
          From the initial concept stage to the final product delivery, we
          actively seek input from our clients, incorporating their ideas and
          preferences into the design process. This collaborative approach not
          only ensures that our designs meet the needs and expectations of our
          clients but also fosters a sense of ownership and partnership.
        </p>
      </>
    ),
    badge: "Integrity over Profits",
    image: "/interfaces.png",
  },
  {
    title: "Customer Support is Key",
    description: (
      <>
        <hr className="my-10" />
        <p className="text-content">
          In addition to prioritizing customer feedback, we also place a strong
          emphasis on ongoing support and assistance. Our dedicated team is
          always available to address any questions or concerns that may arise,
          providing prompt and personalized assistance to ensure a smooth and
          seamless experience for our clients. Whether it&apos;s troubleshooting
          technical issues or providing guidance on best practices, we are
          committed to going above and beyond to support our clients every step
          of the way.
        </p>
        <hr className="my-10" />
        <p className="text-content">
          Ultimately, our interface design services at Pankh.AI are a reflection
          of our unwavering commitment to honesty, integrity, and customer
          satisfaction. By placing the needs of our clients above our bottom
          line and prioritizing transparency, collaboration, and support, we
          strive to deliver exceptional results that not only meet but exceed
          the expectations of our clients. With Pankh.AI, you can trust that
          your interface design needs are in good hands, and that we will stop
          at nothing to ensure your success.
        </p>
      </>
    ),
    badge: "Honesty over Bottom-Line",
    image: "/mobile_screens.png",
  },
  {
    title: "Final Thoughts",
    description: (
      <>
        <hr className="my-10" />
        <p className="text-content">
          In conclusion, Pankh.AI&apos;s interface design consultancy services
          embody a holistic approach that prioritizes honesty, integrity, and
          customer-centricity above all else. With a deep commitment to
          understanding user needs, fostering open communication, and providing
          unwavering support, our team strives to deliver interface designs that
          not only captivate users visually but also enhance functionality and
          usability. By placing a premium on collaboration, transparency, and
          customer feedback, we aim to forge lasting partnerships with our
          clients and empower them to achieve their digital goals with
          confidence. With Pankh.AI, you can trust that your interface design
          needs are met with excellence and dedication, ensuring a seamless and
          impactful user experience every step of the way.
        </p>
      </>
    ),
    badge: "Loyalty over Money",
    image: "/vegeta.jpeg",
  },
];

export const customModelsContent: any = {
  product_first_para: (
    <p>
      At Pankh.AI, we understand that one size does not fit all when it comes to
      AI-driven conversational agents. That&apos;s why we empower businesses to
      bring their own data and train custom LLMs that reflect their brand voice,
      industry expertise, and customer preferences. By leveraging proprietary
      data sets and advanced machine learning techniques, our custom LLMs
      deliver highly personalized and contextually relevant responses, enhancing
      user engagement and satisfaction.
    </p>
  ),
  product_second_para: (
    <p>
      But our custom LLMs are more than just chatbots – they&apos;re intelligent
      assistants that evolve over time. With built-in mechanisms for continuous
      learning and adaptation, our LLMs leverage cached results and real-time
      feedback to refine their understanding, optimize performance, and deliver
      increasingly accurate and impactful interactions. Whether it&apos;s
      answering customer inquiries, providing product recommendations, or
      automating routine tasks, Pankh.AI&apos;s custom LLMs are poised to
      transform the way businesses engage with their audience.
    </p>
  ),
  product_third_para: (
    <p>
      Pankh.AI&apos;s Custom Large Language Models (LLMs) are the next frontier in
      AI-driven conversational technology. Designed to empower businesses with
      unparalleled customization and control, our custom LLMs revolutionize the
      way organizations interact with their customers, clients, and
      stakeholders. Whether you&apos;re a small startup looking to personalize
      customer support or a large enterprise seeking to streamline operations,
      Pankh.AI&apos;s custom LLMs offer a versatile solution tailored to your unique
      needs and requirements.
    </p>
  ),
  use_cases_second_para: (
    <p>
      In addition to bringing your own data and training a custom model,
      leveraging cached results through vector databases offers a substantial
      advantage in enhancing the model&apos;s intelligence with each iteration.
      By storing previous interactions, queries, and responses in vector
      databases, the custom model gains access to a wealth of historical
      knowledge and insights.
      <hr className="my-10" />
      This enables the model to learn from past experiences, refine its
      understanding of user preferences, and improve the accuracy and relevance
      of future responses. Moreover, cached results provide a mechanism for
      continuous learning and adaptation. As the model interacts with users and
      receives feedback, it can dynamically update its knowledge base and adjust
      its behavior based on real-world usage patterns. This iterative learning
      process ensures that the model evolves over time, becoming increasingly
      intelligent and proficient at handling a wide range of queries and
      scenarios.
    </p>
  ),
  use_cases_first_para: (
    <p>
      Bringing your own data and training a custom model offers several distinct
      advantages over making OpenAI API calls. Firstly, it allows for
      unparalleled customization and control over the model&apos;s behavior and
      performance. By training the model on proprietary data sets tailored to
      specific use cases or industries, users can fine-tune the model to better
      understand the nuances of their domain, resulting in more accurate and
      contextually relevant responses.
      <hr className="my-10" />
      Additionally, training a custom model enables organizations to safeguard
      sensitive data and maintain greater privacy and security compared to
      relying on external APIs. Furthermore, owning and training a custom model
      mitigates the risk of disruptions or changes to third-party APIs,
      providing greater reliability and continuity in service. Overall, bringing
      your own data and training a custom model empowers users with flexibility,
      customization, and data ownership, offering a superior alternative to
      relying solely on external API calls.
    </p>
  ),
  use_cases_third_para: (
    <p>
      Furthermore, leveraging cached results offers efficiency gains by reducing
      the need for redundant computations and API calls. Instead of
      recalculating responses from scratch, the model can quickly retrieve
      relevant information from the vector database, resulting in faster
      response times and improved user experience.
      <hr className="my-10" />
      Overall, incorporating cached results from vector databases into the
      training process enhances the capabilities of custom models, enabling them
      to learn from past interactions, adapt to changing conditions, and deliver
      more personalized and contextually relevant responses with each iteration.
      This iterative learning approach not only enhances the intelligence and
      performance of the model but also contributes to a more seamless and
      engaging user experience.
    </p>
  ),
};

export const ourOfferingsContent = [
  {
    title:"What Are AI Models",
    description:(
      <>
      <p className="text-content">
      Artificial intelligence models are computer programs that aim to replicate aspects of human intelligence. Developers input rules (known as algorithms) that allow the program to make decisions, notice patterns, and make predictions.
      Successful models have a user-friendly interface. That means new users can interact with it without much direction.
      For example, Bing Chat is an AI-powered chatbot app that can have back-and-forth conversations with users:
      People type messages into the text box and the software replies—thanks to the accessible interface.
      However, it’s the AI model that does the heavy lifting. It runs in the background and provides relevant answers to questions it has never encountered before.
      Users don’t interact with the AI model directly. But it powers the whole experience.
      Artificial intelligence is a complex topic with a lot of overlapping terminology. So, let’s clear a few things up.
       
</p>
      </>
    )
  },
  {
    title:"Artificial Intelligence vs. Machine Learning vs. Deep Learning",
    description:(
      <>
      <p className="text-content">
      Think of artificial intelligence, machine learning, and deep learning as one big tree. 

The trunk is AI. And one of its biggest branches is machine learning (ML). But that big branch splits into several smaller branches. One of them is deep learning (DL).

What’s the bottom line?

All are connected. But each term doesn’t refer to the same process.

Here’s what it looks like:
       
</p>
      
      </>
    ),
    image:"/AiModels.png"
  },
  {
    title:"Neural Network vs. Deep Learning",
    description:(
      <>
      <p className="text-content">
      Neural Networks and Deep Learning in particular are the bedrocks on which much of modern AI applications are built, including facial recognition software, spam detection, sentiment analysis, chat bots, image generators and more. You may have heard one or both of these words used in relation to artificial intelligence and wondered what they are, and the truth is, they are pretty much different ways to describe the same thing.
      This image shows a simplified network architecture for a basic neural network compared to a deep learning neural network.
       
</p>

      <p className="text-content">
      Input data is connected to a “hidden layer” of nodes. Each node has an associated weight. The weights are multiplied by the incoming data and then submitted to an output layer where an activation function[1] is applied to the incoming values and results in an output result. (Output could be a categorization (e.g., cat or dog), a predicted value (e.g., forward 1-month return for a stock), or a matrix of data that can be translated into an image, for a few examples.
      The real difference between neural networks and deep learning (or deep neural networks) is the number of hidden layers between the input data and the output results. Models referred to as deep learning have many hidden layers whereas a neural network may have as few as one hidden layer. The algorithms[2] by which they can be optimized are the same.
       
</p>
      
      </>
    ),
    image:"/Neural-Network-versus-Deep-Learning.png"
  },
  {
    title:" Supervised vs. Unsupervised Learning",
    description:(
      <>
      <p className="text-content">
      There are two primary motivating reasons why an analyst might train a machine learning model: (1) to try to predict some future event or state or (2) to better understand a set of data. Supervised machine learning methods are often used for purpose 1 and unsupervised machine learning methods are often used for purpose 2.
      This image shows the generalized algorithmic flow for a supervised machine learning method and an unsupervised machine learning method.
       
</p>
      
      <p className="text-content">
      <b>Supervised</b> machine learning models are provided with the “answers” by the human initiating the model.  These models are generally attempting to return a predicted result as close as possible to the “right answer” (or “actual result”) provided in the training data. An example of a supervised model would be a neural network trained to read handwriting. Each image this neural network trains on would have an associated label or category that the algorithm is trying to accurately match. For example, an image of a handwritten 9 would be associated with the supplied label “9”.
      The flow of the modeling in supervised learning is that the model is provided with input data, it applies a function or formula to the incoming data to produce an output answer, the output answer is compared to the human-supplied answers (or the “right answers”), a loss is computed, and this loss is then fed back into the model to adjust the function weights to hopefully produce a more accurate answer (or smaller loss) the next time through.
       
</p>

      <p className="text-content">
      <b>Unsupervised</b> machine learning models are not provided with “answers” but rather seek to uncover underlying patterns within the supplied data. An example of an unsupervised model would be nearest neighbor clustering – using supplied data, the algorithm attempts to group the records into a targeted number of clusters where the members of the cluster are as similar as possible. In this case, the human initiating the model does not have any preconceived notion of what the cluster membership should look like; in other words, the model isn’t attempting to replicate a “right answer”.
      The flow of the modeling in unsupervised learning removes the “my answers” part from the supervised learning flow. Instead, some loss function—such as variance in the output value among group members—is applied directly to the model’s answers and fed back into the modeling process until no further improvements can be made.
       
</p>
      </>
    ),
    image:"/Supervised-vs-Unsupervised-Learning.png"
  },
  {
    title:"Training, Validation and Testing Data",
    description:(
      <>
      <p className="text-content">
      To avoid overfitting their models to the training dataset, quantitative analysts will generally split their data into three distinct sets:
       
</p>
      <p className="text-content"><b>Training data:</b> This is the data that the machine learning algorithm will use to develop a model that hopefully makes accurate predictions or generates some other useful output.
       
</p>

      <p className="text-content"><b>Validation data:</b> Many machine learning algorithms will use validation data for determining whether they have reached a decent stopping point, generally comparing losses on the validation data in the current stage to losses in the prior stage.
       
</p>

      <p className="text-content"><b>Testing data:</b> Machine learning algorithms won’t always hold out a third data set for testing, but when they do, the reason is generally because the algorithm itself has associated “hyper parameters” – values related to the speed at which losses are incorporated into weight changes. To ensure that those parameters aren’t overfit to the training/validation data, a third dataset will be held out to ensure the model works out of sample.
       
</p>

      <p className="text-content">
      To provide a more specific example of how one might bucket a dataset into three parts like this, suppose you wanted to build a model on S&P 500 data from 2013 – 2023 to attempt to predict its price level one month out. I’d start by taking all the dates from 2020 onward and putting those into a “Test” dataset. Then, I’d go through the remaining daily records from 2013 – 2020 and flip a coin for each date. If heads, I’d put them in the “Train” dataset and if tails, I’d put them in the “Validation” dataset. (Of course I would not literally be flipping a coin but would use a computer algorithm to virtually flip the coin).
      This image shows how a sample of 10 data records might be split up and assigned to various training/testing/validation datasets.
       
</p>
      <p className="text-content">
      There are a few ways the data may be split into these 1-3 buckets. A simple randomization process is the most straightforward. However, it may be desirable to split datasets sequentially in time so the out- of-sample validation or testing data also occurs during a time period the model didn’t see in the training data. This can provide greater confidence that the algorithm will work in a real-world environment.
       
</p>
      <p className="text-content">
      One of the greatest pitfalls to using a model to predict some future or unknown event is it will be too biased toward the data it was trained on (the in-sample data) and therefore not work in a real-world setting. In fact, a model that has a large number of parameters (such as a deep learning model) might possibly “memorize” the training dataset. This would look extremely accurate during training and likely wildly inaccurate when implemented in a real-world environment. Thoughtful sample design can help mitigate these problems
       
</p>
      
      </>
    ),
    image:"/Training-Formatted.png"
  },
  {
    title:"Optimizer",
    description:(
      <>
      <p className="text-content">
      An optimizer is an algorithm used in machine learning and artificial intelligence to take the output from a loss function and translate it into changes in model weights (hopefully in a direction that reduces the amount of loss computed at the next step). Think of the optimizer like a golf coach. They watch the player hit several balls and have them make some slight tweaks here and there to their stance, hand positioning, club speed, etc. Then they will observe some more swings and make further tweaks as necessary, always with an eye towards maximizing distance and accuracy.
      This image shows a generalized process flow for how an optimizer uses the data provided to develop a formula that produces “accurate” output (accurate as defined by the human designing the model).
       
</p>

    

      <p className="text-content">
      The way the various machine learning methods differ is primarily through their optimization algorithms. Neural net optimizers implement gradient descent through backpropagation as their optimizer (a future part 2 of this blog post might cover these terms in more detail). For genetic algorithms, the optimizer takes the most successful formulas (those with the lowest computed loss) of a given generation of model and “breeds” and “mutates” them in an effort to find a model that results in less loss than the prior one.
      Particularly in deep learning, but also in other machine learning approaches, optimizers often have parameters associated with them (referred to as hyper parameters). These parameters determine how weights are adjusted in proportion to the computed loss.
       
</p>
      
      
      </>
    ),
    image:"/Optimizer-Formatted.png"
  },
  {
    title:" Loss Function",
    description:(
      <>
      <p className="text-content">
      A loss function is a mathematical formula that is used to compute the difference between the model’s prediction and the “right answers” for a training dataset (assuming the model is supervised).
      This image shows the relationship between the weight assigned to a variable and the resulting “loss” computation for the model. The dots with numbers show where weights were set for subsequent training steps 1 through 5 and their resulting loss.
      
       
</p>

      
      <p className="text-content">

      This image shows the relationship between the weight assigned to a variable and the resulting “loss” computation for the model. The dots with numbers show where weights were set for subsequent training steps 1 through 5 and their resulting loss.
      
      Loss functions are generally matched to the specific optimization approach being taken as well as the type of prediction being made. There are often deep-in-the-weeds mathematical reasons for preferring one loss function over another, but for the layperson the important things to know are that:
       
</p>  
      
      <p className="text-content">
      1. The loss function is usually measuring the difference between the model prediction and the “right answers”,
      
</p>

      <p className="text-content">
      2. Machine learning models are nearly always aiming to minimize the loss, and
      
</p>
      <p className="text-content">
      3. The change in loss from one training cycle to the next usually governs when the machine learning algorithm will halt training.
      
</p>

      <p className="text-content ">
      Note how in the image above, an optimizer will not always find the absolute best weight settings to achieve globally minimal loss. At step four, the size of the weight adjustment was not large enough to find the global minimum, but a potentially adequate local minimum is found instead.
       
</p>
      </>
    ),
    image:"/Loss-Function-Diagram.png"
  }
]

